{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1.8.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage\n",
    "from skimage import exposure, io\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(dev)\n",
    "print(torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image path\n",
    "img_path_train=\"../train/\"\n",
    "img_path_valid=\"../val/\"\n",
    "\n",
    "\n",
    "# figure size \n",
    "figsize = 512\n",
    "\n",
    "# figure size in dataloader\n",
    "figsize2=512\n",
    "\n",
    "# figure size for the correction of the position \n",
    "figsize3 = 512\n",
    "cutwidth = figsize3//20\n",
    "\n",
    "\n",
    "# batch size \n",
    "batchsize=4\n",
    "\n",
    "# epoch\n",
    "epoch_init = 0\n",
    "epoch_max = 10\n",
    "\n",
    "# encoder\n",
    "encoder='resnet18'\n",
    "#encoder='resnet34'\n",
    "#encoder='resnet50'\n",
    "#encoder='resnet101'\n",
    "#encoder='resnet151'\n",
    "\n",
    "# weights\n",
    "encoder_weights='imagenet'\n",
    "#encoder_weights='ssl'\n",
    "#encoder_weights='swsl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img,mn=0,mx=100):\n",
    "    mx1 = np.max(img).astype(np.float32)\n",
    "    mn1 = np.min(img).astype(np.float32)\n",
    "    img = (img.astype(np.float32)-mn1)/(mx1-mn1)\n",
    "#    img = exposure.equalize_adapthist(img.astype(np.uint16))\n",
    "#    img = exposure.equalize_hist(img)\n",
    "\n",
    "    i, j = np.percentile(img, (mn,mx))\n",
    "    img = exposure.rescale_intensity(img, in_range=(i,j)).astype(np.float32)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all(img1, img2):\n",
    "    img1_p = preprocess(img1, mn=0, mx=100)\n",
    "    img2_p = preprocess(img2, mn=1, mx=80)\n",
    "\n",
    "    img1_t=torch.from_numpy(img1_p)\n",
    "    img2_t=torch.from_numpy(img2_p)\n",
    "\n",
    "    img1_t = img1_t.unsqueeze(0).unsqueeze(0)\n",
    "    img2_t = img2_t.unsqueeze(0).unsqueeze(0)\n",
    "    return img1_t, img2_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "imglist = sorted(glob.glob(img_path_train+\"*.tif\"))\n",
    "\n",
    "image_truth_train = []\n",
    "image_input_train = []\n",
    "\n",
    "    \n",
    "j=0\n",
    "for i in tqdm(imglist):\n",
    "#    print(i)\n",
    "    if j%4 == 0:\n",
    "        img1 = io.imread(i).astype(np.float32)\n",
    "        img1 = cv2.resize(img1, (figsize3,figsize3))\n",
    "        i2 = i\n",
    "        j=j+1\n",
    "        continue\n",
    "    elif j%4 == 1:\n",
    "        img2 = io.imread(i).astype(np.float32)\n",
    "        img2 = cv2.resize(img2, (figsize3,figsize3))\n",
    "        j=j+1\n",
    "    elif j%4 == 2:\n",
    "        img2 = io.imread(i).astype(np.float32)\n",
    "        img2 = cv2.resize(img2, (figsize3,figsize3))\n",
    "        j=j+2\n",
    "    else:\n",
    "        print('error: j={}'.format(j))\n",
    "        \n",
    "    \n",
    "    img1_h, img1_w = img1.shape\n",
    "    img2_h, img2_w = img2.shape\n",
    "        \n",
    "    img1_p = preprocess(img1)\n",
    "    img2_p = preprocess(img2, mn=1, mx=80)\n",
    "\n",
    "    image_truth_train.append(img1_p)\n",
    "    image_input_train.append(img2_p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  7.93it/s]\n"
     ]
    }
   ],
   "source": [
    "imglist = sorted(glob.glob(img_path_valid+\"*.tif\"))\n",
    "\n",
    "image_truth_valid = []\n",
    "image_input_valid = []\n",
    "    \n",
    "j=0\n",
    "for i in tqdm(imglist):\n",
    "    if j%4 == 0:\n",
    "        img1 = io.imread(i).astype(np.float32)\n",
    "        img1 = cv2.resize(img1, (figsize3,figsize3))\n",
    "        i2 = i\n",
    "        j=j+1\n",
    "        continue\n",
    "    elif j%4 == 1:\n",
    "        img2 = io.imread(i).astype(np.float32)\n",
    "        img2 = cv2.resize(img2, (figsize3,figsize3))\n",
    "        j=j+1\n",
    "    elif j%4 == 2:\n",
    "        img2 = io.imread(i).astype(np.float32)\n",
    "        img2 = cv2.resize(img2, (figsize3,figsize3))\n",
    "        j=j+2\n",
    "    else:\n",
    "        print('error: j={}'.format(j))\n",
    "        \n",
    "        \n",
    "    img1_p = preprocess(img1)\n",
    "    img2_p = preprocess(img2, mn=1, mx=80)\n",
    "\n",
    "\n",
    "    image_truth_valid.append(img1_p)\n",
    "    image_input_valid.append(img2_p)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "trans2_valid = A.Compose([\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_train_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,img_in,img_th):\n",
    "        self.img_in = img_in\n",
    "        self.img_th = img_th\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return(self.img_in.__len__())\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        transformed = transform(image=self.img_in[idx], mask=self.img_th[idx])\n",
    "        x = transformed['image']\n",
    "        y = transformed['mask']\n",
    "        y = y.unsqueeze(0)\n",
    "        return x, y\n",
    "\n",
    "train_dataset = My_train_dataset(image_input_train, image_truth_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_valid_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,img_in,img_th):\n",
    "        self.img_in = img_in\n",
    "        self.img_th = img_th\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return(self.img_in.__len__())\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x = self.img_in[idx]\n",
    "        y = self.img_th[idx]\n",
    "        \n",
    "        transformed = trans2_valid(image=x)\n",
    "        x = transformed['image']\n",
    "        transformed =trans2_valid(image=y)\n",
    "        y = transformed['image']\n",
    "        return x, y\n",
    "\n",
    "valid_dataset = My_valid_dataset(image_input_valid, image_truth_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__len__(), valid_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batchsize)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(encoder, in_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 2022-07-26 14:51:45.485583: 0.0001 0.7412 0.2703\n",
      "Epoch 2 2022-07-26 14:51:45.756867: 0.0001 0.6967 0.3771\n",
      "Epoch 3 2022-07-26 14:51:46.036672: 0.0001 0.6608 0.4503\n",
      "Epoch 4 2022-07-26 14:51:46.305525: 0.0001 0.6476 0.5204\n",
      "Epoch 5 2022-07-26 14:51:46.577028: 0.0001 0.6144 0.5834\n",
      "Epoch 6 2022-07-26 14:51:46.847632: 0.0001 0.559 0.6137\n",
      "Epoch 7 2022-07-26 14:51:47.130790: 0.0001 0.5632 0.6261\n",
      "Epoch 8 2022-07-26 14:51:47.405477: 0.0001 0.5106 0.6288\n",
      "Epoch 9 2022-07-26 14:51:47.672136: 0.0001 0.5028 0.6222\n",
      "Epoch 10 2022-07-26 14:51:47.953168: 0.0001 0.4664 0.6197\n"
     ]
    }
   ],
   "source": [
    "res_c_tloss = []\n",
    "res_c_vloss = []\n",
    "\n",
    "model = model.to(dev)\n",
    "model.train()\n",
    "\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "c_vloss_best=100.\n",
    "\n",
    "for i in range(epoch_init+1, epoch_max+1):\n",
    "    c_tloss = 0.\n",
    "    for i2, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        inputs, labels = inputs.to(dev), labels.to(dev)\n",
    "        outputs = model(inputs)\n",
    "        train_loss = loss_fn(outputs, labels)\n",
    "        c_loss = train_loss\n",
    "        c_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        c_tloss += c_loss.item()\n",
    "        \n",
    "    model.eval()\n",
    "    c_vloss=0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(dev), labels.to(dev)\n",
    "            outputs = model(inputs)\n",
    "            valid_loss = loss_fn(outputs, labels)\n",
    "            c_loss = valid_loss\n",
    "            c_vloss += c_loss.item()\n",
    "\n",
    "    res_c_tloss.append(c_tloss/len(train_loader))\n",
    "    res_c_vloss.append(c_vloss/len(valid_loader))\n",
    "    print('Epoch {} {}: {:.4} {:.4} {:.4}'.format(i, datetime.datetime.now(), optimizer.param_groups[0]['lr'], c_tloss/len(train_loader), c_vloss/len(valid_loader)))\n",
    "\n",
    "    model.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
